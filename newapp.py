import json
import numpy as np
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModel
from flask import Flask, request, jsonify
from flask_cors import CORS
from underthesea import word_tokenize
import re
import os
import faiss
import pandas as pd
from tensorflow.keras.optimizers import Adam # type: ignore
from tensorflow.keras.regularizers import l2 # type: ignore
# from google.cloud import vision
import io
import os
import base64
import sys
# from model.ocr_module import detect_text
from topics.topics import topics
from bai_tap.bien_doi_deu_van_dung import  generate_problem_and_solution
from bai_tap.thang_deu_van_dung import generate_problem_and_solution_2
from bai_tap.roi_tu_do_van_dung import generate_problem_and_solution_3
from bai_tap.thang_deu_van_dung_cao import generate_problem_and_solution_5
from bai_tap.bien_doi_deu_van_dung_cao import generate_problem_and_solution_6
from bai_tap.nem_xien_van_dung import generate_problem_and_solution_7

app = Flask(__name__)
CORS(app)  # Enable CORS for all routes


class PhoBERTLayer(tf.keras.layers.Layer):
    def __init__(self, phobert_model_name="vinai/phobert-base", **kwargs):
        super(PhoBERTLayer, self).__init__(**kwargs)
        self.phobert_model_name = phobert_model_name
        self.phobert_model = TFAutoModel.from_pretrained(phobert_model_name)

    def call(self, inputs):
        input_ids, attention_mask = inputs
        outputs = self.phobert_model(input_ids=input_ids, attention_mask=attention_mask)
        return outputs.last_hidden_state

    def get_config(self):
        config = super(PhoBERTLayer, self).get_config()
        config.update({"phobert_model_name": self.phobert_model_name})
        return config

    @classmethod
    def from_config(cls, config):
        phobert_model_name = config.pop("phobert_model_name", "vinai/phobert-base")
        config.pop("phobert_model", None)
        return cls(phobert_model_name=phobert_model_name, **config)

# Initialize and load PhoBERT model before using the custom layer
phobert_model_name = "vinai/phobert-base"
phobert_model = TFAutoModel.from_pretrained(phobert_model_name)
tokenizer = AutoTokenizer.from_pretrained(phobert_model_name)

# Define custom objects
custom_objects = {"PhoBERTLayer": PhoBERTLayer}

# Load the model
model = tf.keras.models.load_model('model/physics_model.keras', custom_objects=custom_objects)

# Load the label dictionary
with open('model/physics_label_dict.json', 'r') as f:
    label_dict = json.load(f)

# Set max length for input sequences
max_len = 128

def encode_texts(texts, tokenizer, max_len):
    encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=max_len, return_tensors='tf')
    return encodings['input_ids'], encodings['attention_mask']

uniform_keywords = ["váº­n tá»‘c khÃ´ng Ä‘á»•i", "chuyá»ƒn Ä‘á»™ng tháº³ng Ä‘á»u", "tá»‘c Ä‘á»™ trung bÃ¬nh", "tá»‘c Ä‘á»™ khÃ´ng Ä‘á»•i", "chuyá»ƒn Ä‘á»™ng Ä‘á»u", "khÃ´ng thay Ä‘á»•i tá»‘c Ä‘á»™"]
accelerated_keywords = ["gia tá»‘c", "váº­n tá»‘c thay Ä‘á»•i", "tÄƒng tá»‘c", "biáº¿n Ä‘á»•i", "cháº­m dáº§n Ä‘á»u", "nhanh dáº§n Ä‘á»u", "gia tá»‘c khÃ´ng Ä‘á»•i", "váº­n tá»‘c ban Ä‘áº§u", "váº­n tá»‘c cuá»‘i", "thá»i gian Ä‘áº¡t Ä‘Æ°á»£c váº­n tá»‘c", "hÃ£m phanh"]

def keyword_features(text, uniform_keywords, accelerated_keywords):
    uniform_count = sum([keyword in text for keyword in uniform_keywords])
    accelerated_count = sum([keyword in text for keyword in accelerated_keywords])
    return uniform_count, accelerated_count

def adjust_probabilities(text, probabilities, uniform_keywords, accelerated_keywords):
    uniform_count, accelerated_count = keyword_features(text, uniform_keywords, accelerated_keywords)
    adjustment_factor = 2

    uniform_label_index = label_dict.get('chuyá»ƒn Ä‘á»™ng tháº³ng Ä‘á»u')
    accelerated_label_index = label_dict.get('chuyá»ƒn Ä‘á»™ng tháº³ng biáº¿n Ä‘á»•i Ä‘á»u')

    if uniform_count > 0 and uniform_label_index is not None:
        probabilities[uniform_label_index] *= adjustment_factor
    if accelerated_count > 0 and accelerated_label_index is not None:
        probabilities[accelerated_label_index] *= adjustment_factor

    probabilities /= probabilities.sum()
    return probabilities


def remove_html(txt):
    return re.sub(r'<[^>]*>', '', txt)

def loaddicchar():
    dic = {}
    char1252 = 'aÌ€|aÌ|aÌ‰|aÌƒ|aÌ£|Ã¢Ì€|Ã¢Ì|Ã¢Ì‰|Ã¢Ìƒ|Ã¢Ì£|ÄƒÌ€|ÄƒÌ|ÄƒÌ‰|ÄƒÌƒ|ÄƒÌ£|eÌ€|eÌ|eÌ‰|eÌƒ|eÌ£|ÃªÌ€|ÃªÌ|ÃªÌ‰|ÃªÌƒ|ÃªÌ£|iÌ€|iÌ|iÌ‰|iÌƒ|iÌ£|oÌ€|oÌ|oÌ‰|oÌƒ|oÌ£|Ã´Ì€|Ã´Ì|Ã´Ì‰|Ã´Ìƒ|Ã´Ì£|Æ¡Ì€|Æ¡Ì|Æ¡Ì‰|Æ¡Ìƒ|Æ¡Ì£|uÌ€|uÌ|uÌ‰|uÌƒ|uÌ£|Æ°Ì€|Æ°Ì|Æ°Ì‰|Æ°Ìƒ|Æ°Ì£|yÌ€|yÌ|yÌ‰|yÌƒ|yÌ£|AÌ€|AÌ|AÌ‰|AÌƒ|AÌ£|Ã‚Ì€|Ã‚Ì|Ã‚Ì‰|Ã‚Ìƒ|Ã‚Ì£|Ä‚Ì€|Ä‚Ì|Ä‚Ì‰|Ä‚Ìƒ|Ä‚Ì£|EÌ€|EÌ|EÌ‰|EÌƒ|EÌ£|ÃŠÌ€|ÃŠÌ|ÃŠÌ‰|ÃŠÌƒ|ÃŠÌ£|IÌ€|IÌ|IÌ‰|IÌƒ|IÌ£|OÌ€|OÌ|OÌ‰|OÌƒ|OÌ£|Ã”Ì€|Ã”Ì|Ã”Ì‰|Ã”Ìƒ|Ã”Ì£|Æ Ì€|Æ Ì|Æ Ì‰|Æ Ìƒ|Æ Ì£|UÌ€|UÌ|UÌ‰|UÌƒ|UÌ£|Æ¯Ì€|Æ¯Ì|Æ¯Ì‰|Æ¯Ìƒ|Æ¯Ì£|YÌ€|YÌ|YÌ‰|YÌƒ|YÌ£'.split('|')
    charutf8 = "Ã |Ã¡|áº£|Ã£|áº¡|áº§|áº¥|áº©|áº«|áº­|áº±|áº¯|áº³|áºµ|áº·|Ã¨|Ã©|áº»|áº½|áº¹|á»|áº¿|á»ƒ|á»…|á»‡|Ã¬|Ã­|á»‰|Ä©|á»‹|Ã²|Ã³|á»|Ãµ|á»|á»“|á»‘|á»•|á»—|á»™|á»|á»›|á»Ÿ|á»¡|á»£|Ã¹|Ãº|á»§|Å©|á»¥|á»«|á»©|á»­|á»¯|á»±|á»³|Ã½|á»·|á»¹|á»µ|Ã€|Ã|áº¢|Ãƒ|áº |áº¦|áº¤|áº¨|áºª|áº¬|áº°|áº®|áº²|áº´|áº¶|Ãˆ|Ã‰|áºº|áº¼|áº¸|á»€|áº¾|á»‚|á»„|á»†|ÃŒ|Ã|á»ˆ|Ä¨|á»Š|Ã’|Ã“|á»|Ã•|á»Œ|á»’|á»|á»”|á»–|á»˜|á»œ|á»š|á»|á» |á»¢|Ã™|Ãš|á»¦|Å¨|á»¤|á»ª|á»¨|á»¬|á»®|á»°|á»²|Ã|á»¶|á»¸|á»´".split('|')
    for i in range(len(char1252)):
        dic[char1252[i]] = charutf8[i]
    return dic

dicchar = loaddicchar()

def convert_unicode(txt):
    return re.sub('|'.join(dicchar.keys()), lambda x: dicchar[x.group()], txt)

def chuan_hoa_dau_tu_tieng_viet(word):
    bang_nguyen_am = [
        ["a", "Ã ", "Ã¡", "áº£", "Ã£", "áº¡"],
        ["Äƒ", "áº±", "áº¯", "áº³", "áºµ", "áº·"],
        ["Ã¢", "áº§", "áº¥", "áº©", "áº«", "áº­"],
        ["e", "Ã¨", "Ã©", "áº»", "áº½", "áº¹"],
        ["Ãª", "á»", "áº¿", "á»ƒ", "á»…", "á»‡"],
        ["i", "Ã¬", "Ã­", "á»‰", "Ä©", "á»‹"],
        ["o", "Ã²", "Ã³", "á»", "Ãµ", "á»"],
        ["Ã´", "á»“", "á»‘", "á»•", "á»—", "á»™"],
        ["Æ¡", "á»", "á»›", "á»Ÿ", "á»¡", "á»£"],
        ["u", "Ã¹", "Ãº", "á»§", "Å©", "á»¥"],
        ["Æ°", "á»«", "á»©", "á»­", "á»¯", "á»±"],
        ["y", "á»³", "Ã½", "á»·", "á»¹", "á»µ"]
    ]
    nguyen_am_to_ids = {}
    for i in range(len(bang_nguyen_am)):
        for j in range(len(bang_nguyen_am[i])):
            nguyen_am_to_ids[bang_nguyen_am[i][j]] = (i, j)

    dau_cau = 0
    nguyen_am_index = []
    qu_or_gi = False
    chars = list(word)
    for index, char in enumerate(chars):
        x, y = nguyen_am_to_ids.get(char, (-1, -1))
        if x == -1:
            continue
        if x == 9:
            if index != 0 and chars[index - 1] == 'q':
                chars[index] = 'u'
                qu_or_gi = True
        elif x == 5:
            if index != 0 and chars[index - 1] == 'g':
                chars[index] = 'i'
                qu_or_gi = True
        if y != 0:
            dau_cau = y
            chars[index] = bang_nguyen_am[x][0]
        if not qu_or_gi or index != 1:
            nguyen_am_index.append(index)

    if not nguyen_am_index:
        return word

    if len(nguyen_am_index) < 2:
        if qu_or_gi:
            if len(chars) == 2:
                x, y = nguyen_am_to_ids.get(chars[1])
                chars[1] = bang_nguyen_am[x][dau_cau]
            else:
                x, y = nguyen_am_to_ids.get(chars[2], (-1, -1))
                if x != -1:
                    chars[2] = bang_nguyen_am[x][dau_cau]
                else:
                    chars[1] = bang_nguyen_am[5][dau_cau] if chars[1] == 'i' else bang_nguyen_am[9][dau_cau]
        else:
            x, y = nguyen_am_to_ids.get(chars[nguyen_am_index[0]])
            chars[nguyen_am_index[0]] = bang_nguyen_am[x][dau_cau]
    else:
        if len(nguyen_am_index) == 2 and nguyen_am_index[-1] == len(chars) - 1:
            x, y = nguyen_am_to_ids.get(chars[nguyen_am_index[0]])
            chars[nguyen_am_index[0]] = bang_nguyen_am[x][dau_cau]
        else:
            x, y = nguyen_am_to_ids.get(chars[nguyen_am_index[1]])
            chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]
    return ''.join(chars)

def chuan_hoa_dau_cau_tieng_viet(sentence):
    words = sentence.split()
    for index, word in enumerate(words):
        prefix = re.match(r'^\W+', word)
        suffix = re.match(r'\W+$', word)
        prefix = prefix.group() if prefix else ''
        suffix = suffix.group() if suffix else ''
        cw = re.sub(r'^\W+|\W+$', '', word)
        if len(cw) == 0:
            continue
        cw = chuan_hoa_dau_tu_tieng_viet(cw)
        words[index] = prefix + cw + suffix
    return ' '.join(words)

def remove_punctuation(txt):
    return re.sub(r'[^\w\s]', '', txt)

def chuan_hoa_khoang_trang(txt):
    return re.sub(r'\s+', ' ', txt).strip()

def tokenize(txt):
    return ' '.join(word_tokenize(txt))

def clean_text(text):
    text = remove_html(text)
    text = convert_unicode(text)
    text = chuan_hoa_dau_cau_tieng_viet(text)
    text = text.lower()
    text = remove_punctuation(text)
    text = chuan_hoa_khoang_trang(text)
    text = tokenize(text)
    return text


# def detect_text(image_content, credentials_path):
#     os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = credentials_path
#     client = vision.ImageAnnotatorClient()
#     image = vision.Image(content=image_content)
#     response = client.text_detection(image=image)
#     texts = response.text_annotations

#     if texts:
#         full_text = texts[0].description
#         return full_text
#     else:
#         return None



class Retriever:
    def __init__(self, knowledge_base_path, tokenizer, phobert_model):
        self.knowledge_base = pd.read_csv(knowledge_base_path)  # Äá»c tá»« file CSV
        self.tokenizer = tokenizer
        self.phobert_model = phobert_model
        self.index = faiss.IndexFlatL2(768)
        self.build_index()

    def build_index(self):
        embeddings = []
        for problem in self.knowledge_base['problem']:
            input_ids = self.tokenizer(problem, return_tensors='tf', padding=True, truncation=True, max_length=128)['input_ids']
            embedding = self.phobert_model(input_ids)[0]
            embedding = tf.reduce_mean(embedding, axis=1).numpy()
            embeddings.append(embedding)
        embeddings = np.vstack(embeddings)
        self.index.add(embeddings)

    def retrieve(self, query, top_k=5):
        input_ids = self.tokenizer(query, return_tensors='tf', padding=True, truncation=True, max_length=128)['input_ids']
        query_embedding = self.phobert_model(input_ids)[0]
        query_embedding = tf.reduce_mean(query_embedding, axis=1).numpy()
        D, I = self.index.search(query_embedding, top_k)
        return self.knowledge_base.iloc[I[0]]

# Khá»Ÿi táº¡o Retriever vá»›i Ä‘Æ°á»ng dáº«n tá»›i file CSV
tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base")
phobert_model = TFAutoModel.from_pretrained("vinai/phobert-base")
retriever = Retriever('knowledge_base.csv', tokenizer, phobert_model)

# HÃ m predict_problem sá»­ dá»¥ng Retriever
def predict_problem(problem):
    retrieved_problems = retriever.retrieve(problem)
    augmented_input = " ".join([problem] + retrieved_problems['problem'].tolist())
    input_ids, attention_mask = encode_texts([augmented_input], tokenizer, max_len)
    prediction = model.predict({'input_ids': input_ids, 'attention_mask': attention_mask})
    probabilities = prediction[0]
    adjusted_probabilities = adjust_probabilities(problem, probabilities, uniform_keywords, accelerated_keywords)
    predicted_label_index = np.argmax(adjusted_probabilities)
    predicted_label = list(label_dict.keys())[list(label_dict.values()).index(predicted_label_index)]
    return predicted_label

user_states = {}

@app.route('/chat', methods=['POST'])
def chat():
    global user_states
    data = request.get_json()

    user_id = data.get('userId')
    if not user_id:
        return jsonify({'error': 'Thiáº¿u userId'}), 400

    if user_id not in user_states:
        user_states[user_id] = {
            "mode": "normal",
            "predicted_label": None,
            "difficulty_level": None
        }

    state = user_states[user_id]

    if 'message' in data:
        message = data['message'].lower()
    # elif 'image' in data:
    #     image_data = base64.b64decode(data['image'])
    #     image_content = io.BytesIO(image_data).read()
    #     credentials_path = 'rapid-stage-425307-j4-58d15bd4cd2e.json'  # Thay báº±ng Ä‘Æ°á»ng dáº«n thá»±c táº¿ cá»§a báº¡n
    #     message = detect_text(image_content, credentials_path)
    #     if not message:
    #         return jsonify({'error': 'KhÃ´ng thá»ƒ nháº­n dáº¡ng vÄƒn báº£n tá»« áº£nh'}), 400
    else:
        return jsonify({'error': 'Thiáº¿u thÃ´ng Ä‘iá»‡p hoáº·c áº£nh'}), 400

    if state["mode"] == "normal":
        for keyword, response in topics.items():
            if keyword in message:
                if keyword == "bÃ i táº­p":
                    state["mode"] = "predict_physics"
                    return jsonify({"response": "HÃ£y gá»­i bÃ i táº­p cho tÃ´i."})
                else:
                    return jsonify({"response": response})
        return jsonify({"response": "ChÃ o báº¡n, tÃ´i chÆ°a Ä‘Æ°á»£c há»c kiáº¿n thá»©c nÃ y, náº¿u cáº§n táº¡o bÃ i táº­p hÃ£y tÃ¬m tÃ´i nhÃ©"})
    
    elif state["mode"] == "predict_physics":
        cleaned_text = clean_text(message)
        predicted_label = predict_problem(cleaned_text)
        state["mode"] = "ask_difficulty_level"
        state["predicted_label"] = predicted_label  
        return jsonify({"response": f"Káº¿t quáº£ dá»± Ä‘oÃ¡n: {predicted_label}. Báº¡n muá»‘n táº¡o bÃ i á»Ÿ má»©c Ä‘á»™ nÃ o, tÃ´i cÃ³ thá»ƒ táº¡o á»Ÿ cÃ¡c má»©c sau: thÃ´ng hiá»ƒu, váº­n dá»¥ng, váº­n dá»¥ng cao."})
    

    elif state["mode"] == "ask_difficulty_level":
        difficulty_levels = ["thÃ´ng hiá»ƒu", "váº­n dá»¥ng", "váº­n dá»¥ng cao"]
        stop_word = ["stop"]
        if message in stop_word:
            state["mode"] = "predict_physics"
            return jsonify({"response": f"xin lá»—i vÃ¬ sá»± sai sÃ³t nÃ y, báº¡n hÃ£y thá»­ táº¡o láº¡i má»™t bÃ i táº­p khÃ¡c nhÃ© ğŸ˜‡ğŸ¥ºğŸ˜µ"})
                    
        if message in difficulty_levels:
            state["difficulty_level"] = message
            state["mode"] = "ask_number_of_problems"
            return jsonify({"response": f"Báº¡n muá»‘n táº¡o bao nhiÃªu bÃ i dáº¡ng {state['predicted_label']} á»Ÿ má»©c Ä‘á»™ {state['difficulty_level']}?"})
        
        else:
            return jsonify({"response": "Vui lÃ²ng chá»n má»™t trong cÃ¡c má»©c Ä‘á»™: thÃ´ng hiá»ƒu, váº­n dá»¥ng, váº­n dá»¥ng cao."})

    elif state["mode"] == "ask_number_of_problems":
        try:
            num_problems = int(message)
            # ------------------------------------------------------------------------------------------------------------
            # ---------------------------------------Váº­n dá»¥ng------------------------------------------------------------
            if state["predicted_label"] == "chuyá»ƒn Ä‘á»™ng tháº³ng biáº¿n Ä‘á»•i Ä‘á»u" and state["difficulty_level"] == "váº­n dá»¥ng" and isinstance(num_problems, int):
                problems = []
                for _ in range(num_problems):
                    problem, solution = generate_problem_and_solution()
                    problems.append({"problem": problem, "solution": solution})
                
                state["mode"] = "normal"
                return jsonify({
                    "response": f"ÄÃ£ táº¡o {num_problems} bÃ i táº­p á»Ÿ má»©c Ä‘á»™ váº­n dá»¥ng.",
                    "problems": problems
                })
            
            if state["predicted_label"] == "chuyá»ƒn Ä‘á»™ng tháº³ng Ä‘á»u" and state["difficulty_level"] == "váº­n dá»¥ng" and isinstance(num_problems, int):
                problems = []
                for _ in range(num_problems):
                    problem, solution = generate_problem_and_solution_2()
                    problems.append({"problem": problem, "solution": solution})
                
                state["mode"] = "normal"
                return jsonify({
                    "response": f"ÄÃ£ táº¡o {num_problems} bÃ i táº­p á»Ÿ má»©c Ä‘á»™ váº­n dá»¥ng.",
                    "problems": problems
                })

            if state["predicted_label"] == "chuyá»ƒn Ä‘á»™ng nÃ©m xiÃªn" and state["difficulty_level"] == "váº­n dá»¥ng" and isinstance(num_problems, int):
                problems = []
                for _ in range(num_problems):
                    problem, solution = generate_problem_and_solution_7()
                    problems.append({"problem": problem, "solution": solution})
                
                state["mode"] = "normal"
                return jsonify({
                    "response": f"ÄÃ£ táº¡o {num_problems} bÃ i táº­p á»Ÿ má»©c Ä‘á»™ váº­n dá»¥ng.",
                    "problems": problems
                })

            if state["predicted_label"] == "rÆ¡i tá»± do" and state["difficulty_level"] == "váº­n dá»¥ng" and isinstance(num_problems, int):
                problems = []
                for _ in range(num_problems):
                    problem, solution = generate_problem_and_solution_3()
                    problems.append({"problem": problem, "solution": solution})
                
                state["mode"] = "normal"
                return jsonify({
                    "response": f"ÄÃ£ táº¡o {num_problems} bÃ i táº­p á»Ÿ má»©c Ä‘á»™ váº­n dá»¥ng.",
                    "problems": problems
                })
            

            # ------------------------------------------------------------------------------------------------------------
            # ---------------------------------------váº­n dá»¥ng cao------------------------------------------------------------
       

            if state["predicted_label"] == "chuyá»ƒn Ä‘á»™ng tháº³ng biáº¿n Ä‘á»•i Ä‘á»u" and state["difficulty_level"] == "váº­n dá»¥ng cao" and isinstance(num_problems, int):
                problems = []
                for _ in range(num_problems):
                    problem, solution = generate_problem_and_solution_6()
                    problems.append({"problem": problem, "solution": solution})
                
                state["mode"] = "normal"
                return jsonify({
                    "response": f"ÄÃ£ táº¡o {num_problems} bÃ i táº­p á»Ÿ má»©c Ä‘á»™ váº­n dá»¥ng cao.",
                    "problems": problems
                })
            
            if state["predicted_label"] == "chuyá»ƒn Ä‘á»™ng tháº³ng Ä‘á»u" and state["difficulty_level"] == "váº­n dá»¥ng cao" and isinstance(num_problems, int):
                problems = []
                for _ in range(num_problems):
                    problem, solution = generate_problem_and_solution_5()
                    problems.append({"problem": problem, "solution": solution})
                
                state["mode"] = "normal"
                return jsonify({
                    "response": f"ÄÃ£ táº¡o {num_problems} bÃ i táº­p á»Ÿ má»©c Ä‘á»™ váº­n dá»¥ng cao.",
                    "problems": problems
                })

            if state["predicted_label"] == "chuyá»ƒn Ä‘á»™ng nÃ©m xiÃªn" and state["difficulty_level"] == "váº­n dá»¥ng cao" and isinstance(num_problems, int):
                problems = []
                for _ in range(num_problems):
                    problem, solution = generate_problem_and_solution_7()
                    problems.append({"problem": problem, "solution": solution})
                
                state["mode"] = "normal"
                return jsonify({
                    "response": f"ÄÃ£ táº¡o {num_problems} bÃ i táº­p á»Ÿ má»©c Ä‘á»™ váº­n dá»¥ng cao.",
                    "problems": problems
                })

            if state["predicted_label"] == "rÆ¡i tá»± do" and state["difficulty_level"] == "váº­n dá»¥ng cao" and isinstance(num_problems, int):
                problems = []
                for _ in range(num_problems):
                    problem, solution = generate_problem_and_solution_3()
                    problems.append({"problem": problem, "solution": solution})
                
                state["mode"] = "normal"
                return jsonify({
                    "response": f"ÄÃ£ táº¡o {num_problems} bÃ i táº­p á»Ÿ má»©c Ä‘á»™ váº­n dá»¥ng cao.",
                    "problems": problems
                })
            
            else:
                state["mode"] = "normal"
                return jsonify({"response": "Dáº¡ng bÃ i táº­p khÃ´ng phÃ¹ há»£p hoáº·c má»©c Ä‘á»™ hoáº·c sá»‘ lÆ°á»£ng bÃ i táº­p khÃ´ng há»£p lá»‡."})

        except ValueError:
            return jsonify({"response": "Vui lÃ²ng nháº­p má»™t sá»‘ há»£p lá»‡."})
    
    # Äáº·t láº¡i cháº¿ Ä‘á»™ vá» normal sau khi xá»­ lÃ½ tá»«ng chá»§ Ä‘á» cá»¥ thá»ƒ
    state["mode"] = "normal"
    return jsonify({"response": "Xin chÃ o! Báº¡n cáº§n giÃºp gÃ¬?"})

if __name__ == '__main__':
    port = int(os.environ.get("PORT", 8080))
    app.run(host='0.0.0.0', port=port)